---
title: "Session 4: Variational autoencoders"
author: "Methods seminar: Deep Learning"
date: "01/04/2020"
output:
  html_document:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 4
    toc_float: true
    collapsed: no
    smooth_scroll: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
solution = FALSE
```

***

Based on [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r) [notebook](https://github.com/jjallaire/deep-learning-with-r-notebooks/edit/master/notebooks/8.4-generating-images-with-vaes.Rmd)

***

Required packages for the exercise: 

```{r}

suppressPackageStartupMessages({
  library(keras)
  library(R6)
  library(ggplot2)
})

```

The following code shows in details how VAE is implemented. **You don't need to read/ understand each single line of code** as the exercises in the last sections will only focus on some parts. This session aims to show you how VAE works 'under the hood' but wrappers are of course available online. 

## Default Variational autoencoder

Let's quickly go over a Keras implementation of a VAE. Schematically, it looks like this:

```{r, eval=FALSE}
# Encode the input into a mean and variance parameter
c(z_mean, z_log_variance) %<% encoder(input_img)

# Draws a latent point using a small random epsilon
z <- z_mean + exp(z_log_variance) * epsilon 

# Decodes z back to an image
reconstructed_img <- decoder(z) 

# Creates a model
model <- keras_model(input_img, reconstructed_img)

# Then train the model using 2 losses:
# a reconstruction loss and a regularization loss
```

### Encoder 

In this example we will train our model based on the `MNIST` dataset (handwritten digits): 

```{r}
mnist <- dataset_mnist() 
c(c(x_train, y_train), c(x_test, y_test)) %<-% mnist

x_train <- x_train / 255
x_train <- array_reshape(x_train, dim =c(dim(x_train), 1))

x_test <- x_test / 255
x_test <- array_reshape(x_test, dim =c(dim(x_test), 1))
```

Here is the encoder network we will use: a very simple convnet which maps the input image `x` to two vectors, `z_mean` and `z_log_variance`.

Here we define: 
+ the input layer, which will recieve images of size 28 x 28 x 1 (greyscale color only)
+ convolutional layers, `layer_conv_2d`
+ a flattening layer that will shrink the input to a latent vector. 

```{r}

img_shape <- c(28, 28, 1)
batch_size <- 16
latent_dim <- 2L  # Dimensionality of the latent space: a plane

input_img <- layer_input(shape = img_shape)

x <- input_img %>% 
  layer_conv_2d(filters = 32, # number of output, ie, number of kernels
                kernel_size = 3, # kernel shape 
                padding = "same", # 0s padding to have input shape = output shape
                activation = "relu") %>% 
  layer_conv_2d(filters = 64, kernel_size = 3, padding = "same", 
                activation = "relu", strides = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = 3, padding = "same",
                activation = "relu") %>%
  layer_conv_2d(filters = 64, kernel_size = 3, padding = "same",
                activation = "relu")

shape_before_flattening <- k_int_shape(x)

x <- x %>% 
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu")

z_mean <- x %>% 
  layer_dense(units = latent_dim)

z_log_var <- x %>% 
  layer_dense(units = latent_dim)

encoder <- keras_model(input_img, c(z_mean, z_log_var))
encoder

```

### Latent space

Next is the code for using `z_mean` and `z_log_var`, the parameters of the statistical distribution assumed to have produced `input_img`, to generate a latent space point `z`. Here, you wrap some arbitrary code into a `layer_lambda()`, which wraps our function that samples from the latent space into a layer. 

```{r}
sampling <- function(args) {
  c(z_mean, z_log_var) %<-% args
  epsilon <- k_random_normal(shape = list(k_shape(z_mean)[1], latent_dim),
                             mean = 0, stddev = 1)
  z_mean + k_exp(z_log_var) * epsilon
}

z <- list(z_mean, z_log_var) %>% 
  layer_lambda(sampling)
z
```

### Decoder 

This is the decoder implementation: we reshape the vector `z` to the dimensions of an image, then we use a few convolution layers to obtain a final image output that has the same dimensions as the original `input_img`.

```{r}

# This is the input where we will feed `z`.
decoder_input <- layer_input(k_int_shape(z)[-1])

x <- decoder_input %>% 
  # Upsample to the correct number of units
  layer_dense(units = prod(as.integer(shape_before_flattening[-1])),
              activation = "relu") %>% 
  # Reshapes into an image of the same shape as before the last flatten layer
  layer_reshape(target_shape = shape_before_flattening[-1]) %>% 
  # Applies and then reverses the operation to the initial stack of 
  # convolution layers
  layer_conv_2d_transpose(filters = 32, kernel_size = 3, padding = "same",
                          activation = "relu", strides = c(2, 2)) %>%  
  layer_conv_2d(filters = 1, kernel_size = 3, padding = "same",
                activation = "sigmoid")  
  # We end up with a feature map of the same size as the original input.

# This is our decoder model.
decoder <- keras_model(decoder_input, x)

# We then apply it to `z` to recover the decoded `z`.
z_decoded <- decoder(z) 

decoder
```

The dual loss of a VAE doesn't fit the traditional expectation of a sample-wise function of the form `loss(input, target)`. Thus, we set up a custom function that integrates the two losses. 

```{r}

CustomVariationalLayer <- R6Class("CustomVariationalLayer",
                                  
  inherit = KerasLayer,
  
  public = list(
    
    vae_loss = function(x, z_decoded) { 
      x <- k_flatten(x)
      z_decoded <- k_flatten(z_decoded)
      xent_loss <- metric_binary_crossentropy(x, z_decoded)
      kl_loss <- -5e-4 * k_mean(
        1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), 
        axis = -1L
      )
      k_mean(xent_loss + kl_loss)
    },
    
    call = function(inputs, mask = NULL) {
      x <- inputs[[1]]
      z_decoded <- inputs[[2]]
      loss <- self$vae_loss(x, z_decoded)
      self$add_loss(loss, inputs = inputs)
      x
    }
  )
)

layer_variational <- function(object) { 
  create_layer(CustomVariationalLayer, object, list())
} 

# Call the custom layer on the input and the decoded output to obtain
# the final model output
y <- list(input_img, z_decoded) %>% 
  layer_variational() 

```

Finally, we instantiate and train the model. Since the loss has been taken care of in our custom layer, we don't specify an external loss at compile time (`loss = NULL`), which in turns means that we won't pass target data during training (as you will see, we only pass `x_train` to the model in `fit` ).

```{r}
vae <- keras_model(input_img, y)

vae %>% compile(
  optimizer = "rmsprop",
  loss = NULL, 
  experimental_run_tf_function = FALSE 
)

vae

```

### Training 

For time purpose, we already trained the model so that you can simply load it. 

```{r, echo=TRUE, results='hide', eval=FALSE}

## NOT RUN
history_vae <- vae %>% fit(
  x = x_train, y = NULL,
  epochs = 20,
  batch_size = batch_size,
  validation_data = list(x_test, NULL)
)

vae %>% save_model_weights_tf("def_vae/default_vae")
saveRDS(history_vae, "def_vae/history.rds")
saveRDS(history_vae, "def_vae/history3_5.rds", version = 2) 

```


```{r}

load_model_weights_tf(vae, "def_vae/default_vae")
if (getRversion() < '3.5.0') {
  history_vae <- readRDS("def_vae/history3_5.rds.rds")
} else {
  history_vae <- readRDS("def_vae/history.rds")
}

plot(history_vae)

```

### Image generation 

Once we have our trained model, we can use the `decoder` network to turn arbitrary latent space vectors into images. 

```{r}

n = 15
digit_size = 28
# n  = Number of rows / columns of digits
# digit_size =  Height / width of digits in pixels

# Transforms linearly spaced coordinates on the unit square through the inverse
# CDF (ppf) of the Gaussian to produce values of the latent variables z,
# because the prior of the latent space is Gaussian
grid_x <- qnorm(seq(0.05, 0.95, length.out = n))
grid_y <- qnorm(seq(0.05, 0.95, length.out = n))

op <- par(mfrow = c(n, n), mar = c(0,0,0,0), bg = "black")
for (i in 1:length(grid_x)) {
  yi <- grid_x[[i]]
  for (j in 1:length(grid_y)) {
    xi <- grid_y[[j]]
    z_sample <- matrix(c(xi, yi), nrow = 1, ncol = 2)
    z_sample <- t(replicate(batch_size, z_sample, simplify = "matrix"))
    x_decoded <- decoder %>% predict(z_sample, batch_size = batch_size)
    digit <- array_reshape(x_decoded[1,,,], dim = c(digit_size, digit_size))
    plot(as.raster(digit))
  }
}

```

The grid of sampled digits shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning, e.g. there is a direction for "four-ness", "one-ness", etc. This is one main difference with traditional autoencoders. 

## Exercises

### A. Latent space visualization

Visualize the latent space before and after training. In the same plot, show how the different classes are grouped. 

Tip: use the `encoder` model and the `predict` function. 

```{r eval = solution, include = solution}

## Solution
# recompile the encoder model to have an untrained version
input_img_un <- layer_input(shape = img_shape)
x_un <- input_img_un %>% 
  layer_conv_2d(filters = 32, kernel_size = 3, padding = "same", 
                activation = "relu") %>% 
  layer_conv_2d(filters = 64, kernel_size = 3, padding = "same", 
                activation = "relu", strides = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = 3, padding = "same", 
                activation = "relu") %>%
  layer_conv_2d(filters = 64, kernel_size = 3, padding = "same", 
                activation = "relu") 
x_un <- x_un %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu")
z_mean_un <- x_un %>% 
  layer_dense(units = latent_dim)
z_log_var_un <- x_un %>% 
  layer_dense(units = latent_dim)

encoder_untrained <- keras_model(input_img_un, c(z_mean_un, z_log_var_un))

# compute the middle layer
z_untrained <- encoder_untrained %>% predict(x_test, batch_size = batch_size)
z_trained <- encoder %>% predict(x_test, batch_size = batch_size)

library(patchwork)
lat_space <- data.frame(ls1_untrained = z_untrained[[1]][,1], 
                        ls2_untrained = z_untrained[[1]][,2], 
                        ls1           = z_trained[[1]][,1], 
                        ls2           = z_trained[[1]][,2],
                        num = as.factor(y_test))
gp1 <- ggplot(lat_space, aes(x=ls1_untrained, y=ls2_untrained, color = num)) + geom_point()
gp2 <- ggplot(lat_space, aes(x=ls1, y=ls2, color = num)) + geom_point()
gp1 + gp2

```


### B. Image generation

Produce a new pannel of numbers, this time using a non-normal distribution. 

What changes with the grid of samples shown in the example ? 

```{r, eval = solution, include = solution}

## Solution
# We can reuse the `generate_num` but using for instance a uniform distribution. 
n = 15
digit_size = 28

grid_x <- runif(n)
grid_y <- runif(n)

op <- par(mfrow = c(n, n), mar = c(0,0,0,0), bg = "black")
for (i in 1:length(grid_x)) {
  yi <- grid_x[[i]]
  for (j in 1:length(grid_y)) {
    xi <- grid_y[[j]]
    z_sample <- matrix(c(xi, yi), nrow = 1, ncol = 2)
    z_sample <- t(replicate(batch_size, z_sample, simplify = "matrix"))
    x_decoded <- decoder %>% predict(z_sample, batch_size = batch_size)
    digit <- array_reshape(x_decoded[1,,,], dim = c(digit_size, digit_size))
    plot(as.raster(digit))
  }
}

# By sampling on a non-normal distribution, we miss some classes (e.g. 0, 1, ...)
# and we loose the continuity along the numbers. 

```


### C. KL loss

In this section we will see the importance of the KL loss. 

Recompile the VAE that we created but by removing the KL loss. 

Once you are done, you can load the corresponding VAE's weights that we already trained and generate a new pannel of numbers from it. What changes with the grid of samples shown in the example ? 

```{r eval = solution, include = solution}

## Solution
# Warning, make sure that you didn't overwrite parts of the VAE in exercise 1 (e.g. 'x' or 'z_mean'). 
CustomVariationalLayer <- R6Class("CustomVariationalLayer",
                                  
  inherit = KerasLayer,
  
  public = list(
    
    vae_loss = function(x, z_decoded) {
      x <- k_flatten(x)
      z_decoded <- k_flatten(z_decoded)
      xent_loss <- metric_binary_crossentropy(x, z_decoded)
      # kl_loss <- -5e-4 * k_mean(
      #   1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), 
      #   axis = -1L
      # )
      k_mean(xent_loss) ## <--- no more KL loss in the loss function, only reconstruction loss
    },
    
    call = function(inputs, mask = NULL) {
      x <- inputs[[1]]
      z_decoded <- inputs[[2]]
      loss <- self$vae_loss(x, z_decoded)
      self$add_loss(loss, inputs = inputs)
      x
    }
  )
)

layer_variational <- function(object) { 
  create_layer(CustomVariationalLayer, object, list())
}

y <- list(input_img, z_decoded) %>% 
  layer_variational() 

vae <- keras_model(input_img, y)

vae %>% compile(
  optimizer = "rmsprop",
  loss = NULL, 
  experimental_run_tf_function=FALSE
)

```


```{r, echo=TRUE, results='hide', eval=FALSE}
    
    history_vae <- vae %>% fit(
      x = x_train, y = NULL,
      epochs = 10,
      batch_size = batch_size,
      validation_data = list(x_test, NULL)
    )

vae %>% save_model_weights_tf("noKL_vae/noKL_vae")
saveRDS(history_vae, "noKL_vae/history.rds")
saveRDS(history_vae, "noKL_vae/history3_5.rds", version = 2) 

```


```{r eval = solution, include = solution}

load_model_weights_tf(vae, "noKL_vae/noKL_vae")
if (getRversion() < '3.5.0') {
    history_vae <- readRDS("noKL_vae/history3_5.rds.rds")
} else {
    history_vae <- readRDS("noKL_vae/history.rds")
}
plot(history_vae)

```


```{r eval = solution, include = solution}

n = 15
digit_size = 28 
grid_x <- qnorm(seq(0.05, 0.95, length.out = n))
grid_y <- qnorm(seq(0.05, 0.95, length.out = n))

op <- par(mfrow = c(n, n), mar = c(0,0,0,0), bg = "black")
for (i in 1:length(grid_x)) {
  yi <- grid_x[[i]]
  for (j in 1:length(grid_y)) {
    xi <- grid_y[[j]]
    z_sample <- matrix(c(xi, yi), nrow = 1, ncol = 2)
    z_sample <- t(replicate(batch_size, z_sample, simplify = "matrix"))
    x_decoded <- decoder %>% predict(z_sample, batch_size = batch_size)
    digit <- array_reshape(x_decoded[1,,,], dim = c(digit_size, digit_size))
    plot(as.raster(digit))
  }
}
# Without KL loss, the decoder is not able to retrieve some classes (eg 1, 7, 9).
# we can look at the latent space to see why

```

```{r eval = solution, include = solution}

# We can go back to the latent space representation to see how it changed
z_noKL <- encoder %>% predict(x_test, batch_size = batch_size)

lat_space$ls1_noKL <- z_noKL[[1]][,1]
lat_space$ls2_noKL <- z_noKL[[1]][,2]

gp1 <- ggplot(lat_space, aes(x=ls1, y=ls2, color = num)) + geom_point()
gp2 <- ggplot(lat_space, aes(x=ls1_noKL, y=ls2_noKL, color = num)) + geom_point()
gp1 + gp2
# The latent space is not regularized anymore; the '1' class is not continuous
# with the other classes and the space is not centered around c(0,0). 
# ==> some classes that are far from the center of the reduced space are typically
# the classes that can not be retrieved in the above plot (eg 1, 7, 9)

```


### D. Dimensionality variation

Look at the models output shape and identify which layers are shrinking the input image (and to what dimensionality) and which layers are extending it (again, with the corresponding dimensionality). 

```{r eval = solution, include = solution}

## Solution
# We can look at the Encoder and Decoder summaries: 
encoder
decoder
# where we can see that the following layers modify the dimensionality; 
# - conv2d_1 (Conv2D)
# - flatten (Flatten) 
# - reshape (Reshape) 
# - conv2d_transpose
```


## SessionInfo 


```{r session info}
sessionInfo()
```


